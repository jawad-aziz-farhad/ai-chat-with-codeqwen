🧠 AI-Powered Chat Assistant with Next.js, LangChain, and CodeQwen
This project is a blazing-fast, server-rendered AI chat assistant built with Next.js, powered by LangChain and the Ollama runtime. It uses the CodeQwen language model to generate intelligent, context-aware responses — ideal for building developer tools, knowledge assistants, or AI copilots.

## 🚀 Features

⚡ Next.js App Router with optimized API endpoints

🤖 Integrated LangChain + Ollama for running local or containerized LLMs

🧬 Uses the CodeQwen model for highly capable code understanding and generation

🔄 Supports streaming responses (optional)

💡 Clean, modular architecture for extending chains, memory, or tools

🌐 Works locally with Ollama, no need for external APIs

## 🛠️ Tech Stack

Next.js (App Router, API routes)

LangChain.js

Ollama (local LLM runtime)

CodeQwen model (via Ollama)

Optional: Tailwind CSS for styling, React hooks for UI state

## Getting Started

1. Install Ollama & pull the CodeQwen model:

```bash
ollama pull codeqwen
```

2. Clone this repo & install dependencies:

```bash
git clone https://github.com/jawad-aziz-farhad/ai-chat-with-codeqwen
cd ai-chat-with-codeqwen
npm install
```
