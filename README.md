ğŸ§  AI-Powered Chat Assistant with Next.js, LangChain, and CodeQwen
This project is a blazing-fast, server-rendered AI chat assistant built with Next.js, powered by LangChain and the Ollama runtime. It uses the CodeQwen language model to generate intelligent, context-aware responses â€” ideal for building developer tools, knowledge assistants, or AI copilots.

## ğŸš€ Features

âš¡ Next.js App Router with optimized API endpoints

ğŸ¤– Integrated LangChain + Ollama for running local or containerized LLMs

ğŸ§¬ Uses the CodeQwen model for highly capable code understanding and generation

ğŸ”„ Supports streaming responses (optional)

ğŸ’¡ Clean, modular architecture for extending chains, memory, or tools

ğŸŒ Works locally with Ollama, no need for external APIs

## ğŸ› ï¸ Tech Stack

Next.js (App Router, API routes)

LangChain.js

Ollama (local LLM runtime)

CodeQwen model (via Ollama)

Optional: Tailwind CSS for styling, React hooks for UI state

## Getting Started

1. Install Ollama & pull the CodeQwen model:

```bash
ollama pull codeqwen
```

2. Clone this repo & install dependencies:

```bash
git clone https://github.com/jawad-aziz-farhad/ai-chat-with-codeqwen
cd ai-chat-with-codeqwen
npm install
```
